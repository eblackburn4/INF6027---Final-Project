unwantedtext <- c('Verse','Chorus', 'Intro','1','2','3','4')
texttest <- song_master |>
mutate(lyrics = str_remove_all(lyrics, str_c(unwantedtext, collapse = "|"))) |>
mutate(lyrics = str_replace_all(lyrics, "\\\\n", ""))
View(texttest)
texttest <- song_master |>
mutate(lyrics = str_remove_all(lyrics, str_c(unwantedtext, collapse = "|"))) |>
mutate(lyrics = str_replace_all(lyrics, "\\\\n", " "))
View(texttest)
unwantedtext <- c('Verse','Chorus', 'Intro','1','2','3','4')
texttest <- song_master |>
mutate(lyrics = str_remove_all(lyrics, str_c(unwantedtext, collapse = "|"))) |>
mutate(lyrics = str_replace_all(lyrics, "\\\\n", " "))
#use tidytext's 'unnest tokens' function to split each word into its own row
texttest <- texttest |>
unnest_tokens(word, lyrics)
lexicon <- get_sentiments("bing")
View(texttest)
View(texttest)
unwantedtext <- c('Verse','Chorus', 'Intro','1','2','3','4')
onerowperlyric <- song_master |>
mutate(lyrics = str_remove_all(lyrics, str_c(unwantedtext, collapse = "|"))) |>
mutate(lyrics = str_replace_all(lyrics, "\\\\n", " "))
tokenized_lyrics <- onerowperlyric |>
unnest_tokens(word, lyrics)
View(tokenized_lyrics)
View(texttest)
## ---------------------------
## Purpose of script: Intro to data science final project part I: R Code for sentiment analysis
## of hit song lyrics and comparison to nob hits
## Author: Ned Blackburn
## Date Created: 2024-11-27
options(scipen = 6, digits = 5)
library(tidyverse)
library(hrbrthemes)
library(GGally)
library(ggfortify)
library(tidytext)
setwd("~/Desktop/Intro to DS")
## ------------------------------------------------------------------------
# RQ1: how has the lyrical sentiment of popular songs changed over time?
# RQ2: is there a relationship between lyrical sentiment and musical 'valence'?
## ------------------------------------------------------------------------
# 1. Reading in data ------------------------------------------------------
#read in data for lyrics, track metadata (for release date) and artist metadata (for genre), and drop unneeded columns
track_meta <- read.csv("Data/musicoset_metadata/tracks.csv", sep = "\t") |>
select(!c('album_id', 'track_number'))
song_lyrics <- read.csv("Data/musicoset_songfeatures/lyrics.csv", sep = "\t")
artist_meta <- read_delim('Data/musicoset_metadata/artists.csv',
delim = "\t",          # Main delimiter seems to be tabs based on a visual inspection
escape_double = TRUE,  # Handle stray quotes
col_names = TRUE,
trim_ws = TRUE
)
#read in data for song information (for popularity). This particular CSV isn't formatted properly so will require additional cleaning steps
song_meta <- read_delim(
'Data/musicoset_metadata/songs.csv',
delim = "\t",          # Main delimiter seems to be tabs based on a visual inspection
escape_double = TRUE,  # Handle stray quotes
col_names = TRUE,
trim_ws = TRUE
)
# function to clean extraneous symbols and text from the csv using regex
clean_csv <- function(x) {
if (is.character(x)) {
x <- str_remove_all(x, '^[",]+|[",]+$')
}
return(x)
}
# apply cleaning function to the whole csv and then split the csv manually using known column names from the musicOset data schema
song_meta <- song_meta |>
mutate(across(everything(), clean_csv)) |>
separate(col = names(song_meta)[1],
into = c("song_id", "song_name", "billboard", "artists", "popularity", "explicit", "song_type"),
sep = "\t",
fill = "right",
extra = "merge") |>
select(!c('billboard','explicit'))
#check for na values - all checks return 0 so we're good to proceed
sum(is.na(song_meta))
sum(is.na(song_lyrics))
sum(is.na(track_meta))
#join all datasets on the 'song_id' column and filter out any that don't have lyrics
song_master <- left_join(song_meta, track_meta, by = 'song_id') |>
left_join(song_lyrics, by = 'song_id') |>
filter(lyrics != "")
# 2.Data preprocessing and EDA --------------------------------------------
#remove meta-words and linebreaks in the the lyrics column that are not actually part of the lyrics e.g. 'verse 1'
unwantedtext <- c('Verse','Chorus', 'Intro','1','2','3','4')
onerowperlyric <- song_master |>
mutate(lyrics = str_remove_all(lyrics, str_c(unwantedtext, collapse = "|"))) |>
mutate(lyrics = str_replace_all(lyrics, "\\\\n", " "))
#use tidytext's 'unnest tokens' function to split each word into its own row
tokenized_lyrics <- onerowperlyric |>
unnest_tokens(word, lyrics)
lexicon <- get_sentiments("bing")
View(onerowperlyric)
onerowperlyric <- song_master |>
mutate(lyrics = str_remove_all(lyrics, str_c(unwantedtext, collapse = "|"))) |>
mutate(lyrics = str_replace_all(lyrics, "\\\\n", " ")) |>
mutate(lyrics = str_replace_all(lyrics, "'", ""))
View(onerowperlyric)
tokenized_lyrics <- onerowperlyric |>
unnest_tokens(word, lyrics)
View(tokenized_lyrics)
View(onerowperlyric)
unwantedtext <- c('Verse','Chorus', 'Intro','1','2','3','4','\\')
onerowperlyric <- song_master |>
mutate(lyrics = str_remove_all(lyrics, str_c(unwantedtext, collapse = "|"))) |>
mutate(lyrics = str_replace_all(lyrics, "\\\\n", " ")) |>
mutate(lyrics = str_replace_all(lyrics, "'", ""))
unwantedtext <- c('Verse','Chorus', 'Intro','1','2','3','4')
onerowperlyric <- song_master |>
mutate(lyrics = str_remove_all(lyrics, str_c(unwantedtext, collapse = "|"))) |>
mutate(lyrics = str_replace_all(lyrics, "\\\\n", " ")) |>
mutate(lyrics = str_replace_all(lyrics, "'", "")) |>
mutate(lyrics = str_replace_all(lyrics, "\\",""))
unwantedtext <- c('Verse','Chorus', 'Intro','1','2','3','4', '\\\\')
onerowperlyric <- song_master |>
mutate(lyrics = str_remove_all(lyrics, str_c(unwantedtext, collapse = "|"))) |>
mutate(lyrics = str_replace_all(lyrics, "\\\\n", " ")) |>
mutate(lyrics = str_replace_all(lyrics, "'", ""))
View(onerowperlyric)
unwantedtext <- c('Verse','Chorus', 'Intro','1','2','3','4')
onerowperlyric <- song_master |>
mutate(lyrics = str_remove_all(lyrics, str_c(unwantedtext, collapse = "|"))) |>
mutate(lyrics = str_replace_all(lyrics, "\\\\n", " ")) |>
mutate(lyrics = str_replace_all(lyrics, "'", ""))
View(onerowperlyric)
unwantedtext <- c('Verse','Chorus', 'Intro','1','2','3','4')
onerowperlyric <- song_master |>
mutate(lyrics = str_remove_all(lyrics, str_c(unwantedtext, collapse = "|"))) |>
mutate(lyrics = str_replace_all(lyrics, "\\\\n", " ")) |>
mutate(lyrics = str_replace_all(lyrics, "'", "")) |>
mutate(lyrics = str_replace_all(lyrics,"\\\\", "")
unwantedtext <- c('Verse','Chorus', 'Intro','1','2','3','4')
unwantedtext <- c('Verse','Chorus', 'Intro','1','2','3','4')
onerowperlyric <- song_master |>
mutate(lyrics = str_remove_all(lyrics, str_c(unwantedtext, collapse = "|"))) |>
mutate(lyrics = str_replace_all(lyrics, "\\\\n", " ")) |>
mutate(lyrics = str_replace_all(lyrics, "'", "")) |>
mutate(lyrics = str_replace_all(lyrics, "\\\\" , ""))
View(onerowperlyric)
tokenized_lyrics <- onerowperlyric |>
unnest_tokens(word, lyrics)
View(tokenized_lyrics)
View(tokenized_lyrics)
View(song_master)
hits <- read.csv('Data/hits_dataset.csv')
View(song_meta)
hits <- read.csv('Data/hits_dataset.csv', sep = "\t")
View(hits)
View(track_meta)
View(tokenized_lyrics)
View(lexicon)
sm_tokenized_lyrics_sentiment <- sm_tokenized_lyrics |>
inner_join(lexicon, by = word)
## ---------------------------
## Purpose of script: Intro to data science final project part I: R Code for sentiment analysis
## of hit song lyrics and comparison to nob hits
## Author: Ned Blackburn
## Date Created: 2024-11-27
options(scipen = 6, digits = 5)
library(tidyverse)
library(hrbrthemes)
library(GGally)
library(ggfortify)
library(tidytext)
setwd("~/Desktop/Intro to DS")
## ------------------------------------------------------------------------
# RQ1: how has the lyrical sentiment of popular songs changed over time?
# RQ2: is there a relationship between lyrical sentiment and musical 'valence'?
## ------------------------------------------------------------------------
# 1. Reading in data ------------------------------------------------------
#read in data for lyrics, track metadata (for release date) and artist metadata (for genre), and drop unneeded columns
track_meta <- read.csv("Data/musicoset_metadata/tracks.csv", sep = "\t") |>
select(!c('album_id', 'track_number'))
song_lyrics <- read.csv("Data/musicoset_songfeatures/lyrics.csv", sep = "\t")
artist_meta <- read_delim('Data/musicoset_metadata/artists.csv',
delim = "\t",          # Main delimiter seems to be tabs based on a visual inspection
escape_double = TRUE,  # Handle stray quotes
col_names = TRUE,
trim_ws = TRUE
)
#read in data for song information (for popularity). This particular CSV isn't formatted properly so will require additional cleaning steps
song_meta <- read_delim(
'Data/musicoset_metadata/songs.csv',
delim = "\t",          # Main delimiter seems to be tabs based on a visual inspection
escape_double = TRUE,  # Handle stray quotes
col_names = TRUE,
trim_ws = TRUE
)
# function to clean extraneous symbols and text from the csv using regex
clean_csv <- function(x) {
if (is.character(x)) {
x <- str_remove_all(x, '^[",]+|[",]+$')
}
return(x)
}
# apply cleaning function to the whole csv and then split the csv manually using known column names from the musicOset data schema
song_meta <- song_meta |>
mutate(across(everything(), clean_csv)) |>
separate(col = names(song_meta)[1],
into = c("song_id", "song_name", "billboard", "artists", "popularity", "explicit", "song_type"),
sep = "\t",
fill = "right",
extra = "merge") |>
select(!c('billboard','explicit'))
#check for na values - all checks return 0 so we're good to proceed
sum(is.na(song_meta))
sum(is.na(song_lyrics))
sum(is.na(track_meta))
#join all datasets on the 'song_id' column and filter out any that don't have lyrics
song_master <- left_join(song_meta, track_meta, by = 'song_id') |>
left_join(song_lyrics, by = 'song_id') |>
filter(lyrics != "")
hits <- read.csv('Data/hits_dataset.csv')
unwantedtext <- c('Verse','Chorus', 'Intro','1','2','3','4')
song_master <- song_master |>
mutate(lyrics = str_remove_all(lyrics, str_c(unwantedtext, collapse = "|"))) |>
mutate(lyrics = str_replace_all(lyrics, "\\\\n", " ")) |>
mutate(lyrics = str_replace_all(lyrics, "'", "")) |>
mutate(lyrics = str_replace_all(lyrics, "\\\\" , ""))
#load in sentiment lexicon
lexicon <- get_sentiments("bing")
#use tidytext's 'unnest tokens' function to split each word into its own row, then inner join with the sentiment lexicon
sm_tokenized_lyrics <- song_master |>
unnest_tokens(word, lyrics)
lexicon <- get_sentiments("bing")
#join the sentiment lexicon with the tokenized lyrics
sm_tokenized_lyrics_sentiment <- sm_tokenized_lyrics |>
inner_join(lexicon, by = word)
View(sm_tokenized_lyrics)
sm_tokenized_lyrics_sentiment <- sm_tokenized_lyrics |>
inner_join(lexicon, by = 'word')
View(sm_tokenized_lyrics_sentiment)
sm_tokenized_lyrics |> count(sentiment)
sm_tokenized_lyrics_sentiment |> count(sentiment)
word_counts_posneg <- sm_tokenized_lyrics_sentiment |>
count(word, sentiment, sort = TRUE)
View(word_counts_posneg)
sm_tokenized_lyrics_sentiment <- sm_tokenized_lyrics |>
inner_join(lexicon, by = 'word')  |>
anti_join(stop_words, by = 'word')
word_counts_posneg <- sm_tokenized_lyrics_sentiment |>
count(word, sentiment, sort = TRUE)
View(word_counts_posneg)
word_counts_posneg <- sm_tokenized_lyrics_sentiment |>
count(word, sentiment, sort = TRUE) |>
ggplot(aes(x = sentiment)) +
geom_bar()
plt.show()
word_counts_posneg
View(song_meta)
View(sm_tokenized_lyrics_sentiment)
pos_vs_neg <- word_counts_posneg |>
ggplot(aes(x = sentiment, fill = sentiment)) +
geom_bar() +
labs(title = 'Incidence of positive vs negative words in Billboard-charting songs',
subtitle = '1962-2018',
xlab = 'Sentiment',
ylab = 'Number of unique words',
caption = 'Data from MusicOSet. Sentiment analysis using lexicon from Bing et al. (2004)')+
theme_ipsum_rc()
word_counts_posneg <- sm_tokenized_lyrics_sentiment |>
count(word, sentiment, sort = TRUE)
pos_vs_neg <- word_counts_posneg |>
ggplot(aes(x = sentiment, fill = sentiment)) +
geom_bar() +
labs(title = 'Incidence of positive vs negative words in Billboard-charting songs',
subtitle = '1962-2018',
xlab = 'Sentiment',
ylab = 'Number of unique words',
caption = 'Data from MusicOSet. Sentiment analysis using lexicon from Bing et al. (2004)')+
theme_ipsum_rc()
pos_v_neg
pos_vs_neg
pos_vs_neg <- word_counts_posneg |>
ggplot(aes(x = sentiment, fill = sentiment)) +
geom_bar() +
labs(title = 'Incidence of unique positive and negative words',
subtitle = 'Based on a sample of Billboard 100 charting songs from 1962-2018',
xlab = 'Sentiment',
ylab = 'Number of unique words',
caption = 'Data from MusicOSet. Sentiment analysis using lexicon from Bing et al. (2004)')+
theme_ipsum_rc()
pos_vs_neg
word_counts_posneg <- sm_tokenized_lyrics_sentiment |>
count(word, sentiment, sort = TRUE)
pos_vs_neg <- word_counts_posneg |>
ggplot(aes(x = sentiment, fill = sentiment)) +
geom_bar() +
labs(title = 'Incidence of unique positive and negative words',
subtitle = 'Based on a sample of Billboard 100 charting songs from 1962-2018',
xlab = 'Sentiment',
ylab = 'Number of unique words',
caption = 'Data from MusicOSet. Sentiment analysis using lexicon from Bing et al. (2004)')+
theme_ipsum_rc() +
theme(legend.position = 'none')
pos_vs_neg
pos_vs_neg <- word_counts_posneg |>
ggplot(aes(x = sentiment, fill = sentiment)) +
geom_bar() +
labs(title = 'Incidence of unique positive and negative words',
subtitle = 'Based on a sample of Billboard 100 charting songs from 1962-2018',
xlab = 'Sentiment',
ylab = 'Number of unique words',
caption = 'Data from MusicOSet. Sentiment analysis using lexicon from Bing et al. (2004)')+
theme_ipsum_rc() +
theme(legend.position = 'none',
axis.text = element_text(face = 'bold'))
pos_vs_neg
pos_vs_neg <- word_counts_posneg |>
ggplot(aes(x = sentiment, fill = sentiment)) +
geom_bar() +
labs(title = 'Incidence of unique positive and negative words',
subtitle = 'Based on a sample of Billboard 100 charting songs from 1962-2018',
xlab = 'Sentiment',
ylab = 'Number of unique words',
caption = 'Data from MusicOSet. Sentiment analysis using lexicon from Bing et al. (2004)')+
theme_ipsum_rc() +
theme(legend.position = 'none',
axis.title = element_text(face = 'bold'))
pos_vs_neg
?slice_max
pos_vs_neg <- word_counts_posneg |>
ggplot(aes(x = sentiment, fill = sentiment)) +
geom_bar() +
labs(title = 'Incidence of unique positive and negative words',
subtitle = 'Based on a sample of Billboard 100 charting songs from 1962-2018',
xlab = 'Sentiment',
ylab = 'Number of unique words',
caption = 'Data from MusicOSet. Sentiment analysis using lexicon from Bing et al. (2004)')+
theme_ipsum_rc() +
theme(legend.position = 'none',
axis.title = element_text(face = 'bold')) +
scale_fill_viridis_d()
pos_vs_neg
top10posneg <- words_counts_posneg |>
group_by(sentiment) |>
slice_max(n, n = 10) |>
ggplot(aes(x=word, fill = sentiment)) +
geom_bar() +
facet_wrap(~sentiment) +
coord_flip()
top10posneg <- word_counts_posneg |>
group_by(sentiment) |>
slice_max(n, n = 10) |>
ggplot(aes(x=word, fill = sentiment)) +
geom_bar() +
facet_wrap(~sentiment) +
coord_flip()
top10posneg
top10posneg <- word_counts_posneg |>
group_by(sentiment) |>
slice_max(n, n = 10)
View(top10posneg)
top10posneg <- word_counts_posneg |>
group_by(sentiment) |>
slice_max(n, n = 10) |>
ggplot(aes(x=word, fill = sentiment)) +
geom_col() +
facet_wrap(~sentiment) +
coord_flip()
View(top10posneg)
top10posneg <- word_counts_posneg |>
group_by(sentiment) |>
slice_max(n, n = 10) |>
ggplot(aes(x=word, fill = sentiment)) +
geom_col() +
facet_wrap(~sentiment) +
coord_flip()
top10posneg
top10posneg <- word_counts_posneg |>
group_by(sentiment) |>
slice_max(n, n = 10) |>
ggplot(aes(x=word,y=n, fill = sentiment)) +
geom_() +
facet_wrap(~sentiment) +
coord_flip()
top10posneg <- word_counts_posneg |>
group_by(sentiment) |>
slice_max(n, n = 10) |>
ggplot(aes(x=word,y=n, fill = sentiment)) +
geom_col() +
facet_wrap(~sentiment) +
coord_flip()
top10posneg
top10posneg <- word_counts_posneg |>
group_by(sentiment) |>
slice_max(n, n = 10) |>
ggplot(aes(x=word,y=n, fill = sentiment)) +
geom_col() +
facet_wrap(~sentiment, scales = 'free_y') +
coord_flip()
top10posneg
top10posneg <- word_counts_posneg |>
group_by(sentiment) |>
slice_max(n, n = 10) |>
ggplot(aes(x= reorder(word, n), y=n, fill = sentiment)) +
geom_col() +
facet_wrap(~sentiment, scales = 'free_y') +
coord_flip()
top10posneg
top10posneg <- word_counts_posneg |>
group_by(sentiment) |>
slice_max(n, n = 10) |>
ggplot(aes(x= reorder(word, n), y=n, fill = sentiment)) +
geom_col() +
facet_wrap(~sentiment, scales = 'free_y') +
coord_flip() +
labs(title = 'Most common words by sentiment',
xlab = 'Count',
ylab = '',
caption = 'Data from MusicOSet. Sentiment analysis using lexicon from Bing et al. (2004)')+
theme_ipsum_rc() +
theme(legend.position = 'none',
axis.title = element_text(face = 'bold')) +
scale_fill_viridis_d()
top10posneg
top10posneg <- word_counts_posneg |>
group_by(sentiment) |>
slice_max(n, n = 10) |>
ggplot(aes(x= reorder(word, n), y=n, fill = sentiment)) +
geom_col() +
facet_wrap(~sentiment, scales = 'free_y') +
coord_flip() +
labs(title = 'Most common words by sentiment',
x = 'Count',
y = '',
caption = 'Data from MusicOSet. Sentiment analysis using lexicon from Bing et al. (2004)')+
theme_ipsum_rc() +
theme(legend.position = 'none') +
scale_fill_viridis_d()
top10posneg <- word_counts_posneg |>
group_by(sentiment) |>
slice_max(n, n = 10) |>
ggplot(aes(x= reorder(word, n), y=n, fill = sentiment)) +
geom_col() +
facet_wrap(~sentiment, scales = 'free_y') +
coord_flip() +
labs(title = 'Most common words by sentiment',
x = 'Count',
y = 'Db',
caption = 'Data from MusicOSet. Sentiment analysis using lexicon from Bing et al. (2004)')+
theme_ipsum_rc() +
theme(legend.position = 'none') +
scale_fill_viridis_d()
top10posneg
top10posneg <- word_counts_posneg |>
group_by(sentiment) |>
slice_max(n, n = 10) |>
ggplot(aes(x= reorder(word, n), y=n, fill = sentiment)) +
geom_col() +
facet_wrap(~sentiment, scales = 'free_y') +
coord_flip() +
labs(title = 'Most common words by sentiment',
x = '',
y = 'Word Count',
caption = 'Data from MusicOSet. Sentiment analysis using lexicon from Bing et al. (2004)')+
theme_ipsum_rc() +
theme(legend.position = 'none') +
scale_fill_viridis_d()
top10posneg
top10posneg <- word_counts_posneg |>
group_by(sentiment) |>
slice_max(n, n = 10) |>
ggplot(aes(x= reorder(word, n), y=n, fill = sentiment)) +
geom_col() +
facet_wrap(~sentiment, scales = 'free_y') +
coord_flip() +
labs(title = 'Most common words by sentiment',
x = '',
y = 'Word Count',
caption = 'Data from MusicOSet. Sentiment analysis using lexicon from Bing et al. (2004)')+
theme_ipsum_rc() +
theme(legend.position = 'none',
strip.text = element_text(face = "italic")) +
scale_fill_viridis_d()
top10posneg
top10posneg <- word_counts_posneg |>
group_by(sentiment) |>
slice_max(n, n = 10) |>
ggplot(aes(x= reorder(word, n), y=n, fill = sentiment)) +
geom_col() +
facet_wrap(~sentiment, scales = 'free_y') +
coord_flip() +
labs(title = 'Most common words by sentiment',
x = '',
y = 'Word Count',
caption = 'Data from MusicOSet. Sentiment analysis using lexicon from Bing et al. (2004)')+
theme_ipsum_rc() +
theme(legend.position = 'none',
strip.text = element_text(face = "bold.italic")) +
scale_fill_viridis_d()
top10posneg
summary)
summary(sm_tokenized_lyrics_sentiment$release_date_precision)
distinct(sm_tokenized_lyrics_sentiment$release_date_precision)
summarise(sm_tokenized_lyrics_sentiment$release_date_precision)
summarise(count = n_distinct(sm_tokenized_lyrics_sentiment$release_date_precision))
sm_tokenized_lyrics_sentiment |> summarise(count = n_distinct(release_date_precision))
sm_tokenized_lyrics_sentiment |>
group_by(release_date_precision) |>summarise(count = n_distinct(release_date_precision))
sm_tokenized_lyrics_sentiment |>
group_by(release_date_precision) |>summarise(count = n(release_date_precision))
sm_tokenized_lyrics_sentiment |>
group_by(release_date_precision) |>summarise(count = n())
View(song_master)
song_master <- song_master |>
mutate(release_year = str_sub(release_date,1,4))
View(song_master)
